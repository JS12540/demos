{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2MY4-M1zuhV"
      },
      "source": [
        "\n",
        "\n",
        "#Training a Sarcasm Detection Model using LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-AgItE6z80t"
      },
      "source": [
        "## Download the Dataset\n",
        "\n",
        "First, you will download the JSON file and extract the contents into lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_Wlz9i10Dmn"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr4R0I240GOh"
      },
      "outputs": [],
      "source": [
        "# Load the JSON file\n",
        "import json\n",
        "\n",
        "# Load the JSON file\n",
        "with open(\"./sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "# Initialize the lists\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "# Collect sentences and labels into the lists\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN9-ojV55UCR"
      },
      "source": [
        "## Split the Dataset\n",
        "\n",
        "You will then split the lists into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50H0ZrJf035i"
      },
      "outputs": [],
      "source": [
        "training_size = 20000\n",
        "\n",
        "# Split the sentences into training and testing sets\n",
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "\n",
        "# Split the labels into training and testing sets\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYVNY4tE5YbN"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Next, you will generate the vocabulary and padded sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hodsUZib1Ce7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 10000\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "\n",
        "# Generate the word index dictionary\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Generate and pad the training sequences\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Generate and pad the testing sequences\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Convert the labels lists into numpy arrays\n",
        "training_labels = np.array(training_labels)\n",
        "testing_labels = np.array(testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGwXGIXvFhXW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Parameters\n",
        "embedding_dim = 16 # Define the embedding dimension\n",
        "lstm_dim = 32 # Define the LSTM dimension\n",
        "dense_dim = 24 # Define the dense layer dimension\n",
        "NUM_EPOCHS = 10 # Define the number of epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample input tensor"
      ],
      "metadata": {
        "id": "66rd_X78UL8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sequence length and feature dimension\n",
        "sequence_length = 32  # The number of timesteps in each input sample\n",
        "feature_dim = 16      # The dimensionality of the input features\n",
        "\n",
        "# Create a sample input tensor with random data\n",
        "# The shape is (batch_size, sequence_length, feature_dim)\n",
        "# 'batch_size' can be any integer, representing the number of samples\n",
        "sample_input = np.random.rand(1, max_length).astype(np.float32)\n",
        "\n",
        "# Convert the numpy array to a TensorFlow tensor\n",
        "sample_input_tensor = tf.convert_to_tensor(sample_input)"
      ],
      "metadata": {
        "id": "4vzCIcPY7fcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ivy"
      ],
      "metadata": {
        "id": "QQXBYLcB8sZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ivy\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "JrtzgugW8peT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition with LSTM\n"
      ],
      "metadata": {
        "id": "Hkz8H0fHUQGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition with LSTM\n",
        "\n",
        "\n",
        "tf_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 100, input_length=max_length), # Embedding layer\n",
        "    tf.keras.layers.LSTM(lstm_dim), # LSTM layer\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),  # Dense layer with ReLU activation\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid') # Output layer with sigmoid activation\n",
        "])\n",
        "\n",
        "# Build the model\n",
        "tf_lstm.build(sample_input_tensor.shape)"
      ],
      "metadata": {
        "id": "dYJ8jh2n5hFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transpile to torch, ivy.transpile"
      ],
      "metadata": {
        "id": "3scXqwF7WT_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Transpile the model to PyTorch\n",
        " torch_lstm = ivy.transpile(tf_lstm, source=\"tensorflow\", to=\"torch\", args=(sample_input_tensor,))"
      ],
      "metadata": {
        "id": "GFvnbq8CUZkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compile the model"
      ],
      "metadata": {
        "id": "tBtiqkbkW2JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the training parameters\n",
        "tf_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "tf_lstm.summary()"
      ],
      "metadata": {
        "id": "5MYXyB2PUkQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "tf_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "# Train the model\n",
        "history = tf_lstm.fit(\n",
        "    training_padded,\n",
        "    training_labels,\n",
        "    epochs=10,\n",
        "    validation_data=(testing_padded, testing_labels)\n",
        ")\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the total time taken\n",
        "tensorflow_time = end_time - start_time\n",
        "print(f'Total training time: {tensorflow_time:.2f} seconds')"
      ],
      "metadata": {
        "id": "xWEeIIYe5SoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# Convert the features and labels to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(training_padded, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(training_labels, dtype=torch.float32)\n",
        "\n",
        "# Create a Dataset from tensors\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Define a DataLoader with the dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define the loss function (Binary Cross-Entropy for binary classification)\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Define the optimizer (Adam optimizer in this example)\n",
        "optimizer = optim.Adam(torch_lstm.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "LD6wdTiPPVIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Assuming 'torch_lstm' is your model and 'train_loader' is your DataLoader instance\n",
        "for epoch in range(10):\n",
        "    torch_lstm.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = torch_lstm(inputs)  # Forward pass\n",
        "        targets = targets.view(-1, 1)  # Reshape the target to match output shape\n",
        "        loss = loss_function(outputs, targets)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Convert outputs to predicted class (0 or 1) by rounding the sigmoid output\n",
        "        predicted_classes = outputs.round()\n",
        "        correct_predictions += (predicted_classes == targets).sum().item()\n",
        "        total_predictions += targets.size(0)\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    print(f'Epoch {epoch+1} completed, Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the total time taken\n",
        "torch_time = end_time - start_time\n",
        "print(f'Total training time: {total_time:.2f} seconds')\n"
      ],
      "metadata": {
        "id": "0dhkQL3sSVaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Define labels, positions, and time values\n",
        "labels = ['PyTorch', 'TensorFlow']\n",
        "x_pos = np.arange(len(labels))\n",
        "times = [torch_time, tensorflow_time]\n",
        "\n",
        "# Create the bar chart\n",
        "plt.bar(x_pos, times, align='center', alpha=0.7, color=['red', 'blue'])\n",
        "\n",
        "# Add the data values on top of the bars\n",
        "for i, v in enumerate(times):\n",
        "    plt.text(x_pos[i] - 0.1, v + 3, str(v), color='blue', fontweight='bold')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xticks(x_pos, labels)\n",
        "plt.ylabel('Time (seconds)')\n",
        "plt.title('Training Time Comparison')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6syB18Hadpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Ivy framework, the TensorFlow model was seamlessly transpiled into a PyTorch model, allowing for efficient training with similar results to the original TensorFlow model. Remarkably, the training time was significantly reduced from 662.58 seconds to just 264.12 seconds, demonstrating the effectiveness of using Ivy for model conversion and training optimization."
      ],
      "metadata": {
        "id": "_ssK8brgb7xA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etekheWifq9-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "C3_W3_Lab_5_sarcasm_with_bi_LSTM.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}